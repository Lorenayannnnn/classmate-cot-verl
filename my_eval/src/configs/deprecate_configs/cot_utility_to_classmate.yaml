data_args:

#  main_model_name_or_path: open-instruct/outputs/olmo2_rlvr_1b/olmo2_rlvr_1b_20251020_checkpoints
#  main_model_name_or_path: outputs/classmate_cot_w_verl/grpo_olmo_1B_gsm8k_baseline
#  main_model_name_or_path: /local/data/grpo_olmo_1B_with_classmate_llama
#  main_model_name_or_path: outputs/classmate_cot_w_verl/grpo_olmo_1B_gsm8k_baseline
#  main_model_name_or_path: /proj/interaction/interaction-filer/lorena/classmate_cot_w_verl/outputs/grpo_olmo_1B_gsm8k_baseline_400000_episodes
#  main_model_name_or_path: /proj/interaction/interaction-filer/lorena/classmate_cot_w_verl/outputs/grpo_olmo_1B_with_classmate_llama_400000_episodes
#  main_model_name_or_path: /proj/interaction/interaction-filer/lorena/classmate_cot_w_verl/outputs/grpo_olmo_1B_gsm8k_baseline_800000_episodes
#  main_model_name_or_path: /proj/interaction/interaction-filer/lorena/classmate_cot_w_verl/outputs/grpo_olmo_1B_with_classmate_llama_800000_episodes
#  main_model_name_or_path: /proj/interaction/interaction-filer/lorena/classmate_cot_w_verl/outputs/grpo_olmo_1B_with_classmate_llama_reward_weight_1_400000_episodes
#  main_model_name_or_path: LorenaYannnnn/20251210-OLMo-2-1124-7B-DPO_RLVR-GSM-MATH-IF-Mixed-Constraints_baseline_237392_episodes
#  main_model_name_or_path: LorenaYannnnn/20251217-Qwen3-4B-Base_DeepScaleR_baseline_322512_episodes
  main_model_name_or_path: LorenaYannnnn/20251217-Qwen3-4B-Base_DeepScaleR_w_classmate_llama0.5_322512_episodes
  main_model_step_idx: null
  wo_cot: false
  few_shot_k: null
  max_predict_samples: null
  #  openai/gsm8k, EleutherAI/hendrycks_math
  dataset_name: openai/gsm8k
  # EleutherAI/hendrycks_math: (algebra counting_and_probability geometry intermediate_algebra number_theory prealgebra precalculus)
  dataset_subset_name: main
  dataset_split_name: test
  cache_dir: null

model_args:
  # classmate model: Qwen-2.5-7B-Instruct, Llama-3.1-8B-Instruct, mistralai/Ministral-8B-Instruct-2410
  model_name_or_path: Qwen-2.5-7B-Instruct
#  model_name_or_path: Llama-3.1-8B-Instruct
#  model_name_or_path: mistralai/Ministral-8B-Instruct-2410
  use_vllm: True
  vllm_gpu_memory_utilization: 0.9

running_args:
  exp_type: "cot_utility_to_classmate"
  max_response_length: 450    # If step is wo_cot, will be overridden to predefined task's max_response_length, same as the one used for the main model.    # TODO haha change according to training config in verl
  batch_size: 1