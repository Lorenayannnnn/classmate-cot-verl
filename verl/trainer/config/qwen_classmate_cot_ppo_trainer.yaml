# Format checks enforced on CI:
# 1. Comments must appear above each field.
# 2. There must be a blank line between each field.
# 3. Inline comments (after a field on the same line) are not allowed.
# 4. Indentation level is respected for nested fields.

# specify the default per-component configs
defaults:

  # <folder_name>@<field_name>.<field_name>: <yaml_file_name>
  # actor_rollout_ref.actor: trainer/config/actor/dp_actor.yaml
  - actor@actor_rollout_ref.actor: classmate_dp_actor

  # data: trainer/config/data/legacy_data.yaml
  - data@data: qwen_data

  # Reference model config.
  # Reference model will be enabled when actor.use_kl_loss or/and algorithm.use_kl_in_reward is/are True.
  - ref@actor_rollout_ref.ref: dp_ref

  # Rollout model config.
#  - rollout@actor_rollout_ref.rollout: rollout
  - rollout@actor_rollout_ref.rollout: qwen_rollout

  # Model config.
  - model@actor_rollout_ref.model: hf_model

  # Critic model config.
  - critic@critic: dp_critic

  # Reward model config.
  - reward_model@reward_model: dp_reward_model

  # load the reference default config, then apply the fields in the current yaml
  # self config override anything above
  - _self_

# config for actor, rollout and reference model
actor_rollout_ref:

  actor:
    checkpoint:
        save_contents: ['model', 'optimizer', 'extra']
#      save_contents: [ 'hf_model' ]

  # Whether it's a hybrid engine, currently only supports hybrid engine
  hybrid_engine: true

  # Timeout for operations executed against the process group
  nccl_timeout: 600

  # Rollout model config.
  rollout:

    # for huge model, layered summon can save memory (prevent OOM) but make it slower
    layered_summon: False

# custom reward function definition: for compute_score; different from reward manager
custom_reward_function:

  # The path to the file containing your customized reward function.
  # If not specified, pre-implemented reward functions will be used.
#  path: verl/utils/reward_score/qwen_verifier.py
#  path: verl/utils/reward_score/mmlu_sycophancy_verifier.py
  path: verl/utils/reward_score/cot_monitor/BaseVerifier.py

  # The name of the reward function within the specified file. Default is 'compute_score'.
#  name: compute_score
  name: compute_score

reward_model:
#  reward_manager: classmate_cot     # verl/workers/reward_manager/classmate_cot.py
  reward_manager: sycophancy_classmate_cot     # verl/workers/reward_manager/sycophancy_classmate_cot_rm.py

  do_cot_monitor: True
  monitor_template_name: null

  # TODO Change these in keys.py
  monitor_model_name: null
  llm_judge_model_name: null

  classmate_cot_reward_configs:
    # "Qwen/Qwen2.5-7B-Instruct", "mistralai/Ministral-8B-Instruct-2410", "meta-llama/Llama-3.1-8B-Instruct"
    # "meta-llama/Llama-3.2-1B-Instruct", "Qwen/Qwen2.5-1.5B-Instruct", "allenai/OLMo-2-0425-1B-Instruct", "allenai/OLMo-2-0425-1B-SFT"
#    classmate_model_name_or_path_list: ["meta-llama/Llama-3.2-1B-Instruct"]
#    classmate_model_name_or_path_list: ["meta-llama/Llama-3.2-3B-Instruct-Turbo"]
#    classmate_model_name_or_path_list: ["deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"]
#    classmate_model_name_or_path_list: ["Qwen/Qwen2.5-Math-1.5B"]
#    classmate_model_name_or_path_list: ["Qwen/Qwen3-0.6B-Base"]
#    classmate_model_name_or_path_list: ["LorenaYannnnn/20260216-Qwen3-no_nonfactual_irrelevance-0.6B_grpo_warmup_24000_episodes_seed_42"]
#    classmate_model_name_or_path_list: ["Qwen/Qwen3-0.6B"]
#    classmate_model_name_or_path_list: ["LorenaYannnnn/20260217-Qwen3-0.6B_grpo_warmup_16000_episodes_seed_42"]
    classmate_model_name_or_path_list: null

    classmate_reward_weight: 1

    # vanilla_reward, remove_wo_cot, random_truncate, random_truncate_remove_wo_cot, random_truncate_step_wise_utility, help_other_questions, help_other_similar_questions
    classmate_reward_type: vanilla_reward

    #    (use_classmate_main_cond also control add_main_cl_consistency_reward)
    use_classmate_main_cond: no_classmate_when_main_incorrect   # no_classmate (baseline), always, no_classmate_when_main_incorrect, neg_classmate_when_main_incorrect
    token_level_classmate_reward_mode: classmate_partial    # classmate_partial, all
    main_cot_keep_rate: 1   # only effective for vanilla_reward
    main_cot_keep_rate_min: 0.1   # only effective for random_truncate
    main_cot_keep_rate_max: 0.9   # only effective for random_truncate

    add_consistency_reward: False

    # Rollout Importance Sampling: corrects distribution mismatch between rollout and training policies
    # Main control: Upper threshold for IS weights (null = disabled, float = enabled)
    # When enabled, computes IS weights and mismatch metrics (KL, PPL, etc.)
    cl_rollout_is_threshold: 1.4

    # Lower threshold for IS weights (null = auto-reciprocal of upper)
    cl_rollout_is_threshold_lower: 0.6

    # Aggregation level: "token" (biased), "sequence" (unbiased), "geometric" (experimental)
    cl_rollout_is_level: token

    # Bounding mode: "truncate" (cap upper only), "mask" (zero outside bounds)
    cl_rollout_is_mode: truncate

    # Per-token veto threshold for catastrophic outliers
#    cl_rollout_is_veto_threshold: 1e-4
    cl_rollout_is_veto_threshold: null

    # Whether to apply IS weights to policy loss
    # true = apply weights to loss, false = compute metrics only (no weight application)
    # Useful for monitoring mismatch before enabling correction
#    cl_rollout_is: True

    # "immediate_answer", "continue_cot"
    classmate_continue_mode: continue_cot
#    host_classmate_name_to_url_json_fn: "outputs/host_classmate_models/classmate_model_mapping.json"
  classmate_batch_size: 32
#  classmate_free_cache_engine: True
  classmate_use_vllm: True
  classmate_num_return_sequences: 1
  enable_thinking: True
  classmate_generation_configs:
    max_tokens: 4096    # prompt len + main model cot + classmate response len
    do_sample: True
#    max_tokens: 8192    # double if doing help_other_questions
    temperature: 1.0
    top_k: -1
    top_p: 1
    prompt_logprobs: 0

#    do_sample: False   # If false, then temperature and top_p have no effect
#    temperature: 0.0
#    top_p: 0.9
#  classmate_vllm_configs:
#    gpu_memory_utilization: 1
#    tensor_parallel_size: null
#    max_model_len: null
#    enable_sleep_mode: True

  seed: ${data.seed}
  sandbox_fusion_url: null

  llm_judge_model: null
  llm_judge_timeout: 600
  llm_judge_max_tokens: 2048
  llm_judge_max_context_length: 32768
  llm_judge_temperature: 1.0


# config for the algorithm
algorithm:

  # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
  _target_: verl.trainer.config.AlgoConfig

  # Discount factor for future rewards
  gamma: 1.0

  # Trade-off between bias and variance in the GAE estimator
  lam: 1.0

  # Advantage estimator type: "gae", "grpo", "reinforce_plus_plus", etc.
  adv_estimator: gae

  # Whether to normalize advantages by std (specific to GRPO)
  norm_adv_by_std_in_grpo: False

  # Whether to enable in-reward KL penalty
  use_kl_in_reward: False

  # How to estimate KL divergence: "kl", "abs", "mse", "low_var_kl", or "full"
  kl_penalty: kl

  # KL control configuration
  kl_ctrl:

    # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
    _target_: verl.trainer.config.KLControlConfig

    # KL control type: "fixed" or "adaptive"
    type: fixed

    # Initial coefficient for KL penalty
    kl_coef: 0.001

    # Horizon value for adaptive controller (if enabled)
    horizon: 10000

    # Target KL divergence (used for adaptive controller)
    target_kl: 0.1

  # Whether to enable preference feedback PPO
  use_pf_ppo: False

  # Preference feedback PPO settings
  pf_ppo:

    # Method for reweighting samples: "pow", "max_min", or "max_random"
    reweight_method: pow

    # Power used for weight scaling in "pow" method
    weight_pow: 2.0

  # Rollout Importance Sampling: corrects distribution mismatch between rollout and training policies
  # Main control: Upper threshold for IS weights (null = disabled, float = enabled)
  # When enabled, computes IS weights and mismatch metrics (KL, PPL, etc.)
  # TODO haha correspond to clip_ratio_low=0.2 and clip_ratio_high=0.2 in actor.yaml; but here just for logging
  rollout_is_threshold: 1.2

  # Lower threshold for IS weights (null = auto-reciprocal of upper)
  rollout_is_threshold_lower: 0.8

  # Aggregation level: "token" (biased), "sequence" (unbiased), "geometric" (experimental)
  rollout_is_level: token

  # Bounding mode: "truncate" (cap upper only), "mask" (zero outside bounds)
  rollout_is_mode: truncate

  # Per-token veto threshold for catastrophic outliers
#  rollout_is_veto_threshold: 1e-4
  rollout_is_veto_threshold: null

  # Whether to apply IS weights to policy loss
  # true = apply weights to loss, false = compute metrics only (no weight application)
  # Useful for monitoring mismatch before enabling correction
  rollout_is: false

# config for the trainer
trainer:

  # Whether to balance batch sizes across distributed workers
  balance_batch: True

  # Number of epochs in training
  total_epochs: 30

  # Total training steps (can be set explicitly or derived from epochs)
  total_training_steps: null

  # Project name for experiment tracking (e.g., wandb)
  project_name: verl_examples

  # Experiment name for run identification in tracking tools
  experiment_name: gsm8k

  # Logging backends to use: "console", "wandb", etc.
  logger: ["console", "wandb"]

  # Number of generations to log during validation
  log_val_generations: 0

  # Directory for logging rollout data; no dump if null
  rollout_data_dir: null

  # Directory for logging validation data; no dump if null
  validation_data_dir: null

  # Number of nodes used in the training
  nnodes: 1

  # Number of GPUs per node
  n_gpus_per_node: 8

  # Save frequency (by iteration) for model checkpoints
  save_freq: -1

  # ESI refers to the elastic server instance used during training, similar to the training plan. For example,
  # if you purchase 10 hours of computing power, the ESI will automatically shut down after 10 hours of training.
  # To ensure a checkpoint is saved before ESI shuts down, the system will start saving a checkpoint in advance.
  # The advance time is calculated as: Advance Time = Longest historical step duration + Checkpoint save duration + esi_redundant_time.
  # Here, esi_redundant_time is a user-defined value that further extends the advance time for added safety.
  esi_redundant_time: 0

  # Resume mode: "auto", "disable", or "resume_path"
  # "auto": resume from last checkpoint if available
  # "disable": start from scratch
  # "resume_path": resume from a user-defined path
  resume_mode: auto

  # Path to resume training from (only used when resume_mode is "resume_path")
  resume_from_path: null

  # Whether to run validation before training begins
  val_before_train: True

  # Whether to run validation only
  val_only: False

  # Validation frequency (in training iterations)
  test_freq: -1

  # Number of iterations to warm up the critic before updating policy
  critic_warmup: 0

  # Default path to distributed filesystem for saving checkpoints
  default_hdfs_dir: null

  # Whether to delete local checkpoints after loading
  del_local_ckpt_after_load: False

  # Default local directory for saving checkpoints
  # TODO modify
#  default_local_dir: outputs/${trainer.project_name}/${trainer.experiment_name}
  default_local_dir: /proj/interaction/interaction-filer/lorena/${trainer.project_name}/outputs/${trainer.experiment_name}
#  default_local_dir: /local/data/lorena/${trainer.project_name}/outputs/${trainer.experiment_name}

  # Maximum number of actor checkpoints to keep
  max_actor_ckpt_to_keep: null

  # Maximum number of critic checkpoints to keep
  max_critic_ckpt_to_keep: null

  # Timeout (in seconds) for Ray worker to wait for registration
  ray_wait_register_center_timeout: 300

  # Device to run training on (e.g., "cuda", "cpu")
  device: cuda

  # whether to use legacy worker implementation
  #  mode: "auto", "enable", or "disable"
  use_legacy_worker_impl: auto

hydra:
  run:
    dir: ${trainer.default_local_dir}

# profiler configs
global_profiler:

  # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
  _target_: verl.utils.profiler.ProfilerConfig

  # Profiling tool: choose between nsys, npu, torch, torch_memory
  tool: null

  # profile steps
  steps: null

  # Whether to combine continuous steps into one database.
  ## If True, worker.profiler.discrete must be False, [1,2] in one, [5] in another.
  ## If False, [1] in one, [2] in another, [5] in another.
  profile_continuous_steps: False

  # Path to save profiling contents
  # TODO modify
#  save_path: "outputs/profile"
  save_path: "/proj/interaction/interaction-filer/lorena/${trainer.project_name}/outputs/profile"
#  save_path: "/local/data/lorena/${trainer.project_name}/outputs/profile"

  # Specific tool configs, can use +profiler.tool_config.[tool].xxx to config
  global_tool_config:

    # nsys config
    nsys:

      # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
      _target_: verl.utils.profiler.config.NsightToolConfig

      # True for each task has its own database, False for all tasks in one training step share one database.
      discrete: False

      # controller Nvidia Nsight Systems Options. Must set when profile_steps is not None.
      ## reference https://docs.nvidia.com/nsight-systems/UserGuide/index.html
      ## reference https://docs.ray.io/en/latest/ray-observability/user-guides/profiling.html
      controller_nsight_options:

        # Select the API(s) to be traced.
        trace: "cuda,nvtx,cublas,ucx"

        # Track the GPU memory usage by CUDA kernels. Must be string type "true" or "false".
        cuda-memory-usage: "true"

        # CUDA graphs will be traced as a whole
        cuda-graph-trace: "graph"

      # worker Nvidia Nsight Systems Options. Must set when profile_steps is not None.
      worker_nsight_options:

        # Select the API(s) to be traced.
        trace: "cuda,nvtx,cublas,ucx"

        # Track the GPU memory usage by CUDA kernels. Must be string type "true" or "false".
        cuda-memory-usage: "true"

        # CUDA graphs will be traced as a whole
        cuda-graph-trace: "graph"

        # Profiling only in a range of torch.cuda.profiler.start and stop. Do not change this config.
        capture-range: "cudaProfilerApi"

        # Specify the desired behavior when a capture range ends.
        # In verl we need the torch.cuda.profiler.start/stop pair to repeats n times.
        # valid values are "repeat-shutdown:n" or null.
        # For normal whole step profiling, n = len(profile_steps);
        # but for discrete profiling, n = len(profile_steps) * Number(subtasks).
        # Or you can just leave it null and the program will use n = len(profile_steps) * 6;
        capture-range-end: null

        # Send signal to the target application's process group. We let the program to exit by itself.
        kill: none

    # enable memory visualization for debugging memory usage
    torch_memory:

      #  Maximum number of allocation entries to record
      trace_alloc_max_entries: 100_000

      # The depth of the call stack to capture for each allocation
      stack_depth: 32

      # 'alloc': records only allocation events || 'state': records memory state changes || 'all': records both.
      context: "all"

      # 'python': records Python stacks || 'cpp': records C++ stacks (available in some versions) || 'all': records both.
      stacks: "all"

      # devices, record_context etc.
      kw_args: {}

# configs for TransferQueue
transfer_queue:

  # Whether to enable transfer queue
  enable: False

# configs related to ray
ray_kwargs:

  # configs related to ray initialization
  ray_init:

    # Number of CPUs for Ray. Use a fixed number instead of null when using SLURM.
    num_cpus: null

  # Path to save Ray timeline JSON for performance profiling
  timeline_json_file: null
